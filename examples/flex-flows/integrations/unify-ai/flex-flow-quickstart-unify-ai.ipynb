{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with flex flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Learning Objectives** - Upon completing this tutorial, you should be able to:\n",
    "\n",
    "- Write LLM application using notebook and visualize the trace of your application.\n",
    "- Convert the application into a flow and batch run against multi lines of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install dependent packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Trace your application with promptflow\n",
    "\n",
    "Assume we already have a python function that calls Unify AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "\n",
      "from dotenv import load_dotenv\n",
      "from unify import Unify\n",
      "\n",
      "from promptflow.tracing import trace\n",
      "\n",
      "\n",
      "@trace\n",
      "def my_llm_tool(\n",
      "    prompt: str,\n",
      "    # for Unify AI, Model and Provider are to be specified by user.\n",
      "    model_name: str,\n",
      "    provider_name: str,\n",
      "    max_tokens: int = 120,\n",
      "    temperature: float = 1.0,\n",
      ") -> str:\n",
      "    if \"UNIFY_AI_API_KEY\" not in os.environ:\n",
      "        # load environment variables from .env file\n",
      "        load_dotenv()\n",
      "\n",
      "    if \"UNIFY_AI_API_KEY\" not in os.environ:\n",
      "        raise Exception(\"Please specify environment variables: UNIFY_AI_API_KEY\")\n",
      "    messages = [{\"content\": prompt, \"role\": \"system\"}]\n",
      "    api_key = os.environ.get(\"UNIFY_AI_API_KEY\", None)\n",
      "    unify_client = Unify(\n",
      "        api_key=api_key,\n",
      "        model=model_name,\n",
      "        provider=provider_name,\n",
      "    )\n",
      "    response = unify_client.generate(\n",
      "        messages=messages,\n",
      "        max_tokens=int(max_tokens),\n",
      "        temperature=float(temperature),\n",
      "    )\n",
      "\n",
      "    # get first element because prompt is single.\n",
      "    return response\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    result = my_llm_tool(\n",
      "        prompt=\"Write a simple Hello, world! program that displays the greeting message.\",\n",
      "        model_name=\"llama-3.1-8b-chat\",\n",
      "        provider_name=\"together-ai\",\n",
      "    )\n",
      "    print(result)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"llm.py\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: before running below cell, please configure required environment variable `UNIFY_AI_API_KEY` by create an `.env` file. Please refer to `./.env.example` as an template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define provider and model from Unify AI. Refer to https://unify.ai/benchmarks\n",
    "model_name = \"llama-3.1-8b-chat\"\n",
    "provider_name = \"together-ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```\\nprint(\"Hello, world!\")\\n```'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from llm import my_llm_tool\n",
    "\n",
    "# pls configure `UNIFY_AI_API_KEY` environment variable\n",
    "result = my_llm_tool(\n",
    "    prompt=\"Write a simple Hello, world! python program that displays the greeting message when executed. Output code only.\",\n",
    "    model_name=model_name,\n",
    "    provider_name=provider_name,\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize trace by using start_trace\n",
    "\n",
    "Note we add `@trace` in the `my_llm_tool` function, re-run below cell will collect a trace in trace UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```c\\n#include <stdio.h>\\n\\nint main() {\\n    printf(\"Hello, world!\\\\n\");\\n    return 0;\\n}\\n```'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptflow.tracing import start_trace\n",
    "\n",
    "# start a trace session, and print a url for user to check trace\n",
    "start_trace()\n",
    "# rerun the function, which will be recorded in the trace\n",
    "result = my_llm_tool(\n",
    "    prompt=\"Write a simple Hello, world! program that displays the greeting message when executed. Output code only.\",\n",
    "    model_name=model_name,\n",
    "    provider_name=provider_name,\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add another layer of function call. In `programmer.py` there is a function called `write_simple_program`, which calls a new function called `load_prompt` and previous `my_llm_tool` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pathlib import Path\n",
      "from typing import TypedDict\n",
      "\n",
      "from jinja2 import Template\n",
      "from llm import my_llm_tool\n",
      "\n",
      "from promptflow.tracing import trace\n",
      "\n",
      "BASE_DIR = Path(__file__).absolute().parent\n",
      "\n",
      "\n",
      "class Result(TypedDict):\n",
      "    output: str\n",
      "\n",
      "\n",
      "@trace\n",
      "def load_prompt(jinja2_template: str, text: str) -> str:\n",
      "    \"\"\"Load prompt function.\"\"\"\n",
      "    with open(BASE_DIR / jinja2_template, \"r\", encoding=\"utf-8\") as f:\n",
      "        prompt = Template(\n",
      "            f.read(), trim_blocks=True, keep_trailing_newline=True\n",
      "        ).render(text=text)\n",
      "        return prompt\n",
      "\n",
      "\n",
      "@trace\n",
      "def write_simple_program(\n",
      "    text: str = \"Hello World!\",\n",
      "    model_name=\"llama-3.1-8b-chat\",\n",
      "    provider_name=\"together-ai\",\n",
      ") -> Result:\n",
      "    \"\"\"Ask LLM to write a simple program.\"\"\"\n",
      "    prompt = load_prompt(\"hello.jinja2\", text)\n",
      "    output = my_llm_tool(\n",
      "        prompt=prompt,\n",
      "        model_name=model_name,\n",
      "        provider_name=provider_name,\n",
      "        max_tokens=120,\n",
      "    )\n",
      "    return Result(output=output)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    from promptflow.tracing import start_trace\n",
      "\n",
      "    start_trace()\n",
      "    result = write_simple_program(\"Hello, world!\")\n",
      "    print(result)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the programmer.py content\n",
    "with open(\"programmer.py\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=unify-llm-tool&uiTraceId=0x118d5e05554225344dbaed186e3d4d58\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=unify-llm-tool&uiTraceId=0x21c584fafd6393d845c8dc9bad4e5407\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=unify-llm-tool&uiTraceId=0x7f54b359caf9d68702ddfbcd3c88a0b0\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=unify-llm-tool&uiTraceId=0x6dcd0a510e1171662be3e16f98c0b59d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': '```java\\npublic class HelloWorld {\\n  public static void main(String[] args) {\\n    System.out.println(\"Hello, world!\");\\n  }\\n}\\n```'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the flow entry function\n",
    "from programmer import write_simple_program\n",
    "\n",
    "result = write_simple_program(\"Java Hello, world!\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model configuration with environment variables\n",
    "\n",
    "When used in local, create a model configuration object with environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: before running below cell, please configure required environment variable `UNIFY_AI_API_KEY` and `UNIFY_AI_BASE_URL` by creating a `.env` file. Please refer to `./.env.example` as an template.\n",
    "\n",
    "Please refer https://unify.ai/docs/concepts/unify_api.html to get Unify AI Api base url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from promptflow.core import OpenAIModelConfiguration\n",
    "\n",
    "# pls configure `UNIFY_AI_API_KEY`, `UNIFY_AI_BASE_URL` environment variables first\n",
    "if \"UNIFY_AI_API_KEY\" not in os.environ:\n",
    "    # load environment variables from .env file\n",
    "    load_dotenv()\n",
    "\n",
    "if \"UNIFY_AI_API_KEY\" not in os.environ:\n",
    "    raise Exception(\"Please specify environment variables: UNIFY_AI_API_KEY\")\n",
    "model_config = OpenAIModelConfiguration(\n",
    "    base_url=os.environ[\"UNIFY_AI_BASE_URL\"],\n",
    "    api_key=os.environ[\"UNIFY_AI_API_KEY\"],\n",
    "    model=f\"{model_name}@{provider_name}\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correctness': 5,\n",
       " 'readability': 5,\n",
       " 'explanation': \"The code is correct as it is a valid Java program that prints 'Hello, world!' to the console. The readability is also good as the code is properly formatted and follows standard Java conventions.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from code_quality_unify_ai import CodeEvaluator\n",
    "\n",
    "\n",
    "evaluator = CodeEvaluator(model_config=model_config)\n",
    "eval_result = evaluator(result)\n",
    "eval_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch run the function as flow with multi-line data\n",
    "\n",
    "Create a [flow.flex.yaml](https://github.com/microsoft/promptflow/blob/main/examples/flex-flows/basic/flow.flex.yaml) file to define a flow which entry pointing to the python function we defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$schema: https://azuremlschemas.azureedge.net/promptflow/latest/flow.schema.json\n",
      "entry: programmer:write_simple_program\n",
      "environment:\n",
      "  python_requirements_txt: requirements.txt\n",
      "sample:\n",
      "  inputs:\n",
      "    text: Java Hello World!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the flow.flex.yaml content\n",
    "with open(\"flow.flex.yaml\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch run with a data file (with multiple lines of test data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.client import PFClient\n",
    "\n",
    "pf = PFClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-04 20:21:05 +0000][promptflow][WARNING] - Found existing /workspaces/promptflow/examples/flex-flows/unify-llm-tool/flow.flex.yaml, will not respect it in runtime.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=unify_llm_tool_20240804_202105_244263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-04 20:21:05 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run unify_llm_tool_20240804_202105_244263, log path: /root/.promptflow/.runs/unify_llm_tool_20240804_202105_244263/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=unify-llm-tool&uiTraceId=0x5722db5f4be14fd142dd9e29a88ceb10\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=unify-llm-tool&uiTraceId=0x42c6e097bf36c67f3011ac77c96a5ab9\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=unify-llm-tool&uiTraceId=0x079c333a41296e7abe232f38dea00b96\n",
      "2024-08-04 20:21:09 +0000   42488 execution.bulk     INFO     Process 42539 terminated.\n",
      "2024-08-04 20:21:09 +0000   42488 execution.bulk     WARNING  Process 42548 had been terminated.\n",
      "2024-08-04 20:21:09 +0000   42488 execution.bulk     INFO     Process 42533 terminated.\n",
      "2024-08-04 20:21:05 +0000   42203 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2024-08-04 20:21:05 +0000   42203 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3}.\n",
      "2024-08-04 20:21:07 +0000   42203 execution.bulk     INFO     Process name(ForkProcess-4:2)-Process id(42539)-Line number(0) start execution.\n",
      "2024-08-04 20:21:07 +0000   42203 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(42548)-Line number(1) start execution.\n",
      "2024-08-04 20:21:07 +0000   42203 execution.bulk     INFO     Process name(ForkProcess-4:1)-Process id(42533)-Line number(2) start execution.\n",
      "2024-08-04 20:21:08 +0000   42203 execution.bulk     INFO     Process name(ForkProcess-4:2)-Process id(42539)-Line number(0) completed.\n",
      "2024-08-04 20:21:08 +0000   42203 execution.bulk     INFO     Process name(ForkProcess-4:3)-Process id(42548)-Line number(1) completed.\n",
      "2024-08-04 20:21:08 +0000   42203 execution.bulk     INFO     Process name(ForkProcess-4:1)-Process id(42533)-Line number(2) completed.\n",
      "2024-08-04 20:21:08 +0000   42203 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2024-08-04 20:21:08 +0000   42203 execution.bulk     INFO     Average execution time for completed lines: 1.0 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-08-04 20:21:08 +0000   42203 execution.bulk     INFO     The thread monitoring the process [42539-ForkProcess-4:2] will be terminated.\n",
      "2024-08-04 20:21:08 +0000   42203 execution.bulk     INFO     The thread monitoring the process [42548-ForkProcess-4:3] will be terminated.\n",
      "2024-08-04 20:21:08 +0000   42203 execution.bulk     INFO     The thread monitoring the process [42533-ForkProcess-4:1] will be terminated.\n",
      "2024-08-04 20:21:08 +0000   42539 execution.bulk     INFO     The process [42539] has received a terminate signal.\n",
      "2024-08-04 20:21:08 +0000   42548 execution.bulk     INFO     The process [42548] has received a terminate signal.\n",
      "2024-08-04 20:21:08 +0000   42533 execution.bulk     INFO     The process [42533] has received a terminate signal.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"unify_llm_tool_20240804_202105_244263\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-08-04 20:21:05.243352+00:00\"\n",
      "Duration: \"0:00:04.667527\"\n",
      "Output path: \"/root/.promptflow/.runs/unify_llm_tool_20240804_202105_244263\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = \"./data.jsonl\"  # path to the data file\n",
    "# create run with the flow function and data\n",
    "base_run = pf.run(\n",
    "    flow=write_simple_program,\n",
    "    data=data,\n",
    "    column_mapping={\n",
    "        \"text\": \"${data.text}\",\n",
    "    },\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.text</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>inputs.model_name</th>\n",
       "      <th>inputs.provider_name</th>\n",
       "      <th>outputs.output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Python Hello World!</td>\n",
       "      <td>0</td>\n",
       "      <td>llama-3.1-8b-chat</td>\n",
       "      <td>together-ai</td>\n",
       "      <td>```\\n# Print Hello, World! to the console\\npri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C Hello World!</td>\n",
       "      <td>1</td>\n",
       "      <td>llama-3.1-8b-chat</td>\n",
       "      <td>together-ai</td>\n",
       "      <td>```c\\n#include &lt;stdio.h&gt;\\n\\nint main() {\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C# Hello World!</td>\n",
       "      <td>2</td>\n",
       "      <td>llama-3.1-8b-chat</td>\n",
       "      <td>together-ai</td>\n",
       "      <td>```csharp\\nusing System;\\n\\nclass HelloWorld \\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           inputs.text  inputs.line_number  inputs.model_name  \\\n",
       "0  Python Hello World!                   0  llama-3.1-8b-chat   \n",
       "1       C Hello World!                   1  llama-3.1-8b-chat   \n",
       "2      C# Hello World!                   2  llama-3.1-8b-chat   \n",
       "\n",
       "  inputs.provider_name                                     outputs.output  \n",
       "0          together-ai  ```\\n# Print Hello, World! to the console\\npri...  \n",
       "1          together-ai  ```c\\n#include <stdio.h>\\n\\nint main() {\\n    ...  \n",
       "2          together-ai  ```csharp\\nusing System;\\n\\nclass HelloWorld \\...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details = pf.get_details(base_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate your flow\n",
    "Then you can use an evaluation method to evaluate your flow. The evaluation methods are also flows which usually using LLM assert the produced output matches certain expectation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on the previous batch run\n",
    "The **base_run** is the batch run we completed in step 2 above, for web-classification flow with \"data.jsonl\" as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=unify_llm_tool_20240804_202110_069589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-04 20:21:10 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run unify_llm_tool_20240804_202110_069589, log path: /root/.promptflow/.runs/unify_llm_tool_20240804_202110_069589/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=unify-llm-tool&uiTraceId=0x631051c28f5c5d37c0e5d01fa84c93af\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=unify-llm-tool&uiTraceId=0x5138a969d80113a32a121c0fd1d12980\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=unify-llm-tool&uiTraceId=0x0e136f3212c7562a107e6339be97a5b1\n",
      "2024-08-04 20:21:15 +0000   42730 execution.bulk     INFO     Process 42770 terminated.\n",
      "2024-08-04 20:21:15 +0000   42730 execution.bulk     INFO     Process 42786 terminated.\n",
      "2024-08-04 20:21:15 +0000   42730 execution.bulk     WARNING  Process 42776 had been terminated.\n",
      "2024-08-04 20:21:10 +0000   42203 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2024-08-04 20:21:10 +0000   42203 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3}.\n",
      "2024-08-04 20:21:12 +0000   42203 execution.bulk     INFO     Process name(ForkProcess-8:1)-Process id(42770)-Line number(0) start execution.\n",
      "2024-08-04 20:21:12 +0000   42203 execution.bulk     INFO     Process name(ForkProcess-8:2)-Process id(42776)-Line number(2) start execution.\n",
      "2024-08-04 20:21:12 +0000   42203 execution.bulk     INFO     Process name(ForkProcess-8:3)-Process id(42786)-Line number(1) start execution.\n",
      "2024-08-04 20:21:13 +0000   42203 execution.bulk     INFO     Process name(ForkProcess-8:1)-Process id(42770)-Line number(0) completed.\n",
      "2024-08-04 20:21:13 +0000   42203 execution.bulk     INFO     Process name(ForkProcess-8:2)-Process id(42776)-Line number(2) completed.\n",
      "2024-08-04 20:21:14 +0000   42203 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2024-08-04 20:21:14 +0000   42203 execution.bulk     INFO     Average execution time for completed lines: 2.0 seconds. Estimated time for incomplete lines: 2.0 seconds.\n",
      "2024-08-04 20:21:14 +0000   42203 execution.bulk     INFO     Process name(ForkProcess-8:3)-Process id(42786)-Line number(1) completed.\n",
      "2024-08-04 20:21:15 +0000   42203 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2024-08-04 20:21:15 +0000   42203 execution.bulk     INFO     Average execution time for completed lines: 1.67 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-08-04 20:21:15 +0000   42203 execution.bulk     INFO     The thread monitoring the process [42770-ForkProcess-8:1] will be terminated.\n",
      "2024-08-04 20:21:15 +0000   42203 execution.bulk     INFO     The thread monitoring the process [42786-ForkProcess-8:3] will be terminated.\n",
      "2024-08-04 20:21:15 +0000   42203 execution.bulk     INFO     The thread monitoring the process [42776-ForkProcess-8:2] will be terminated.\n",
      "2024-08-04 20:21:15 +0000   42770 execution.bulk     INFO     The process [42770] has received a terminate signal.\n",
      "2024-08-04 20:21:15 +0000   42786 execution.bulk     INFO     The process [42786] has received a terminate signal.\n",
      "2024-08-04 20:21:15 +0000   42776 execution.bulk     INFO     The process [42776] has received a terminate signal.\n",
      "2024-08-04 20:21:16 +0000   42203 execution.bulk     INFO     Executing aggregation function...\n",
      "2024-08-04 20:21:16 +0000   42203 execution.bulk     INFO     Finish executing aggregation function.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"unify_llm_tool_20240804_202110_069589\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-08-04 20:21:10.067782+00:00\"\n",
      "Duration: \"0:00:06.248488\"\n",
      "Output path: \"/root/.promptflow/.runs/unify_llm_tool_20240804_202110_069589\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can also run flow pointing to code evaluator yaml file\n",
    "eval_flow = \"./code-eval-flow.flex.yaml\"\n",
    "\n",
    "eval_run = pf.run(\n",
    "    flow=eval_flow,\n",
    "    init={\"model_config\": model_config},\n",
    "    data=\"./data.jsonl\",  # path to the data file\n",
    "    run=base_run,  # specify base_run as the run you want to evaluate\n",
    "    column_mapping={\n",
    "        \"code\": \"${run.outputs.output}\",\n",
    "    },\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.code</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>outputs.correctness</th>\n",
       "      <th>outputs.readability</th>\n",
       "      <th>outputs.explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>```\\n# Print Hello, World! to the console\\npri...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>The code is correct as it is a simple print st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>```c\\n#include &lt;stdio.h&gt;\\n\\nint main() {\\n    ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>The code is correct as it is a simple C progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>```csharp\\nusing System;\\n\\nclass HelloWorld \\...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>The code is correct as it is a simple 'Hello, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         inputs.code  inputs.line_number  \\\n",
       "0  ```\\n# Print Hello, World! to the console\\npri...                   0   \n",
       "1  ```c\\n#include <stdio.h>\\n\\nint main() {\\n    ...                   1   \n",
       "2  ```csharp\\nusing System;\\n\\nclass HelloWorld \\...                   2   \n",
       "\n",
       "   outputs.correctness  outputs.readability  \\\n",
       "0                    5                    5   \n",
       "1                    5                    4   \n",
       "2                    5                    4   \n",
       "\n",
       "                                 outputs.explanation  \n",
       "0  The code is correct as it is a simple print st...  \n",
       "1  The code is correct as it is a simple C progra...  \n",
       "2  The code is correct as it is a simple 'Hello, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details = pf.get_details(eval_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"average_correctness\": 5.0,\n",
      "    \"average_readability\": 4.333333333333333,\n",
      "    \"total\": 3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "metrics = pf.get_metrics(eval_run)\n",
    "print(json.dumps(metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "The HTML file is generated at '/tmp/pf-visualize-detail-azfuci3g.html'.\n",
      "Trying to view the result in a web browser...\n",
      "Successfully visualized from the web browser.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading \"original-fs\" failed\n",
      "Error: Cannot find module 'original-fs'\n",
      "Require stack:\n",
      "- /vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js\n",
      "\u001b[90m    at Module._resolveFilename (node:internal/modules/cjs/loader:1145:15)\u001b[39m\n",
      "\u001b[90m    at Module._load (node:internal/modules/cjs/loader:986:27)\u001b[39m\n",
      "\u001b[90m    at Module.require (node:internal/modules/cjs/loader:1233:19)\u001b[39m\n",
      "\u001b[90m    at require (node:internal/modules/helpers:179:18)\u001b[39m\n",
      "    at i (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:3:98)\n",
      "    at r.load (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:2:1637)\n",
      "    at h.load (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:1:13958)\n",
      "    at u (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:3:9338)\n",
      "    at Object.errorback (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:3:9457)\n",
      "    at h.triggerErrorback (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:1:14252)\n",
      "    at /vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:1:14003\n",
      "    at r.load (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:2:1654)\n",
      "    at h.load (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:1:13958)\n",
      "    at u (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:3:9338)\n",
      "    at l._loadModule (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:3:9466)\n",
      "    at l._resolve (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:4:452)\n",
      "    at l.defineModule (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:3:5561)\n",
      "    at Function.p [as define] (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:4:1741)\n",
      "    at out-build/bootstrap-amd.js (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:4:6445)\n",
      "    at /vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:1:132\n",
      "    at Object.<anonymous> (/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js:4:9653)\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1358:14)\u001b[39m\n",
      "\u001b[90m    at Module._extensions..js (node:internal/modules/cjs/loader:1416:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1208:32)\u001b[39m\n",
      "\u001b[90m    at Module._load (node:internal/modules/cjs/loader:1024:12)\u001b[39m\n",
      "\u001b[90m    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:174:12)\u001b[39m\n",
      "\u001b[90m    at node:internal/main/run_main_module:28:49\u001b[39m {\n",
      "  code: \u001b[32m'MODULE_NOT_FOUND'\u001b[39m,\n",
      "  requireStack: [\n",
      "    \u001b[32m'/vscode/vscode-server/bin/linux-x64/b1c0a14de1414fcdaa400695b4db1c0799bc3124/out/server-cli.js'\u001b[39m\n",
      "  ],\n",
      "  phase: \u001b[32m'loading'\u001b[39m,\n",
      "  moduleId: \u001b[32m'original-fs'\u001b[39m,\n",
      "  neededBy: [ \u001b[32m'fs'\u001b[39m ]\n",
      "}\n",
      "Here are the modules that depend on it:\n",
      "[ \u001b[32m'fs'\u001b[39m ]\n"
     ]
    }
   ],
   "source": [
    "pf.visualize([base_run, eval_run])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Note\n",
    "\n",
    "By now you've successfully run your simple code generation and evaluation using Unify AI."
   ]
  }
 ],
 "metadata": {
  "build_doc": {
   "author": [
    "D-W-@github.com",
    "wangchao1230@github.com"
   ],
   "category": "local",
   "section": "Flow",
   "weight": 10
  },
  "description": "A quickstart tutorial to run a flex flow and evaluate it.",
  "kernelspec": {
   "display_name": "prompt_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "resources": "examples/requirements.txt, examples/flex-flows/basic"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
