{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with flex flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Learning Objectives** - Upon completing this tutorial, you should be able to:\n",
    "\n",
    "- Write LLM application using notebook and visualize the trace of your application.\n",
    "- Convert the application into a flow and batch run against multi lines of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install dependent packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Trace your application with promptflow\n",
    "\n",
    "Assume we already have a python function that calls Unify AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "\n",
      "from dotenv import load_dotenv\n",
      "from openai.version import VERSION as OPENAI_VERSION\n",
      "\n",
      "from promptflow.tracing import trace\n",
      "\n",
      "\n",
      "def get_client():\n",
      "    if OPENAI_VERSION.startswith(\"0.\"):\n",
      "        raise Exception(\n",
      "            \"Please upgrade your OpenAI package to version >= 1.0.0 or using the command: pip install --upgrade openai.\"\n",
      "        )\n",
      "    api_key = os.environ.get(\"OPENAI_API_KEY\", None)\n",
      "    if api_key:\n",
      "        from openai import OpenAI\n",
      "\n",
      "        return OpenAI()\n",
      "    else:\n",
      "        from openai import AzureOpenAI\n",
      "\n",
      "        return AzureOpenAI(\n",
      "            api_version=os.environ.get(\"OPENAI_API_VERSION\", \"2023-07-01-preview\")\n",
      "        )\n",
      "\n",
      "\n",
      "@trace\n",
      "def my_llm_tool(\n",
      "    prompt: str,\n",
      "    # for AOAI, deployment name is customized by user, not model name.\n",
      "    deployment_name: str,\n",
      "    max_tokens: int = 120,\n",
      "    temperature: float = 1.0,\n",
      "    top_p: float = 1.0,\n",
      "    n: int = 1,\n",
      ") -> str:\n",
      "    if \"OPENAI_API_KEY\" not in os.environ and \"AZURE_OPENAI_API_KEY\" not in os.environ:\n",
      "        # load environment variables from .env file\n",
      "        load_dotenv()\n",
      "\n",
      "    if \"OPENAI_API_KEY\" not in os.environ and \"AZURE_OPENAI_API_KEY\" not in os.environ:\n",
      "        raise Exception(\n",
      "            \"Please specify environment variables: OPENAI_API_KEY or AZURE_OPENAI_API_KEY\"\n",
      "        )\n",
      "    messages = [{\"content\": prompt, \"role\": \"system\"}]\n",
      "    response = get_client().chat.completions.create(\n",
      "        messages=messages,\n",
      "        model=deployment_name,\n",
      "        max_tokens=int(max_tokens),\n",
      "        temperature=float(temperature),\n",
      "        top_p=float(top_p),\n",
      "        n=int(n),\n",
      "    )\n",
      "\n",
      "    # get first element because prompt is single.\n",
      "    return response.choices[0].message.content\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    result = my_llm_tool(\n",
      "        prompt=\"Write a simple Hello, world! program that displays the greeting message.\",\n",
      "        deployment_name=\"gpt-4o\",\n",
      "    )\n",
      "    print(result)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"llm.py\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: before running below cell, please configure required environment variable `UNIFY_API_KEY` by create an `.env` file. Please refer to `./.env.example` as an template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define provider and model from unify. Refer to https://unify.ai/benchmarks\n",
    "model_name = \"llama-3.1-8b-chat\"\n",
    "provider_name = \"octoai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\nprint(\"Hello, world!\")\\n```'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from llm import my_llm_tool\n",
    "\n",
    "# pls configure `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` environment variables first\n",
    "result = my_llm_tool(\n",
    "    prompt=\"Write a simple Hello, world! python program that displays the greeting message when executed. Output code only.\",\n",
    "    model_name=model_name,\n",
    "    provider_name=provider_name,\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize trace by using start_trace\n",
    "\n",
    "Note we add `@trace` in the `my_llm_tool` function, re-run below cell will collect a trace in trace UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```cpp\\n#include <iostream>\\n\\nint main() {\\n    std::cout << \"Hello, world!\" << std::endl;\\n    return 0;\\n}\\n```'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=unify-llm-tool&uiTraceId=0x8b3ca5c573696019dd080376199ba71f\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=unify-llm-tool&uiTraceId=0xfe5c73bc84f9df87d8a792c6b7ae496b\n"
     ]
    }
   ],
   "source": [
    "from promptflow.tracing import start_trace\n",
    "\n",
    "# start a trace session, and print a url for user to check trace\n",
    "start_trace()\n",
    "# rerun the function, which will be recorded in the trace\n",
    "result = my_llm_tool(\n",
    "    prompt=\"Write a simple Hello, world! program that displays the greeting message when executed. Output code only.\",\n",
    "    model_name=model_name,\n",
    "    provider_name=provider_name,\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add another layer of function call. In `programmer.py` there is a function called `write_simple_program`, which calls a new function called `load_prompt` and previous `my_llm_tool` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pathlib import Path\n",
      "from typing import TypedDict\n",
      "\n",
      "from jinja2 import Template\n",
      "from llm import my_llm_tool\n",
      "\n",
      "from promptflow.tracing import trace\n",
      "\n",
      "BASE_DIR = Path(__file__).absolute().parent\n",
      "\n",
      "\n",
      "class Result(TypedDict):\n",
      "    output: str\n",
      "\n",
      "\n",
      "@trace\n",
      "def load_prompt(jinja2_template: str, text: str) -> str:\n",
      "    \"\"\"Load prompt function.\"\"\"\n",
      "    with open(BASE_DIR / jinja2_template, \"r\", encoding=\"utf-8\") as f:\n",
      "        prompt = Template(\n",
      "            f.read(), trim_blocks=True, keep_trailing_newline=True\n",
      "        ).render(text=text)\n",
      "        return prompt\n",
      "\n",
      "\n",
      "@trace\n",
      "def write_simple_program(\n",
      "    text: str = \"Hello World!\", model_name=\"gpt-4\", provider_name=\"openai\"\n",
      ") -> Result:\n",
      "    \"\"\"Ask LLM to write a simple program.\"\"\"\n",
      "    prompt = load_prompt(\"hello.jinja2\", text)\n",
      "    output = my_llm_tool(prompt=prompt, model_name=model_name, provider_name=provider_name, max_tokens=120)\n",
      "    return Result(output=output)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    from promptflow.tracing import start_trace\n",
      "\n",
      "    start_trace()\n",
      "    result = write_simple_program(\"Hello, world!\")\n",
      "    print(result)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the programmer.py content\n",
    "with open(\"programmer.py\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'public class HelloWorld {\\n    public static void main(String[] args) {\\n        System.out.println(\"Hello, world!\");\\n    }\\n}'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the flow entry function\n",
    "from programmer import write_simple_program\n",
    "\n",
    "result = write_simple_program(\"Java Hello, world!\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model configuration with environment variables\n",
    "\n",
    "When used in local, create a model configuration object with environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "if \"AZURE_OPENAI_API_KEY\" not in os.environ:\n",
    "    # load environment variables from .env file\n",
    "    load_dotenv()\n",
    "\n",
    "if \"AZURE_OPENAI_API_KEY\" not in os.environ:\n",
    "    raise Exception(\"Please specify environment variables: AZURE_OPENAI_API_KEY\")\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_deployment=deployment_name,\n",
    "    api_version=\"2023-07-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-28 11:08:34 +0000][promptflow.core._prompty_utils][ERROR] - Exception occurs: RateLimitError: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-07-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 4 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
      "[2024-07-28 11:08:34 +0000][promptflow.core._prompty_utils][WARNING] - RateLimitError #0, Retry-After=4, Back off 4.0 seconds for retry.\n",
      "[2024-07-28 11:08:38 +0000][promptflow.core._prompty_utils][ERROR] - Exception occurs: RateLimitError: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-07-01-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 6 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
      "[2024-07-28 11:08:38 +0000][promptflow.core._prompty_utils][WARNING] - RateLimitError #1, Retry-After=6, Back off 6.0 seconds for retry.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'correctness': 5,\n",
       " 'readability': 4,\n",
       " 'explanation': \"The code is correct as it successfully prints 'Hello, world!' in Java. The readability is slightly reduced due to the JSON format wrapping the code, but the Java code itself is well-formatted and easy to understand.\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import paths  # add the code_quality module to the path\n",
    "from code_quality import CodeEvaluator\n",
    "\n",
    "evaluator = CodeEvaluator(model_config=model_config)\n",
    "eval_result = evaluator(result)\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch run the function as flow with multi-line data\n",
    "\n",
    "Create a [flow.flex.yaml](https://github.com/microsoft/promptflow/blob/main/examples/flex-flows/basic/flow.flex.yaml) file to define a flow which entry pointing to the python function we defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the flow.flex.yaml content\n",
    "with open(\"flow.flex.yaml\") as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch run with a data file (with multiple lines of test data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.client import PFClient\n",
    "\n",
    "pf = PFClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"./data.jsonl\"  # path to the data file\n",
    "# create run with the flow function and data\n",
    "base_run = pf.run(\n",
    "    flow=write_simple_program,\n",
    "    data=data,\n",
    "    column_mapping={\n",
    "        \"text\": \"${data.text}\",\n",
    "    },\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pf.get_details(base_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate your flow\n",
    "Then you can use an evaluation method to evaluate your flow. The evaluation methods are also flows which usually using LLM assert the produced output matches certain expectation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on the previous batch run\n",
    "The **base_run** is the batch run we completed in step 2 above, for web-classification flow with \"data.jsonl\" as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also run flow pointing to yaml file\n",
    "eval_flow = \"../eval-code-quality/flow.flex.yaml\"\n",
    "\n",
    "eval_run = pf.run(\n",
    "    flow=eval_flow,\n",
    "    init={\"model_config\": model_config},\n",
    "    data=\"./data.jsonl\",  # path to the data file\n",
    "    run=base_run,  # specify base_run as the run you want to evaluate\n",
    "    column_mapping={\n",
    "        \"code\": \"${run.outputs.output}\",\n",
    "    },\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pf.get_details(eval_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metrics = pf.get_metrics(eval_run)\n",
    "print(json.dumps(metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.visualize([base_run, eval_run])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "By now you've successfully run your first prompt flow and even did evaluation on it. That's great!\n",
    "\n",
    "You can check out more examples:\n",
    "- [Basic Chat](https://github.com/microsoft/promptflow/tree/main/examples/flex-flows/chat-basic): demonstrates how to create a chatbot that can remember previous interactions and use the conversation history to generate next message."
   ]
  }
 ],
 "metadata": {
  "build_doc": {
   "author": [
    "D-W-@github.com",
    "wangchao1230@github.com"
   ],
   "category": "local",
   "section": "Flow",
   "weight": 10
  },
  "description": "A quickstart tutorial to run a flex flow and evaluate it.",
  "kernelspec": {
   "display_name": "prompt_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "resources": "examples/requirements.txt, examples/flex-flows/basic"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
